{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8f8bKVmPblun0Ywz8YJls",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjani4jnn/PRODIGY_GA_TASKS/blob/main/Task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py0DaLAbm1ap",
        "outputId": "dafc48f2-564d-4524-eca7-d6db716e1283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox jumps over the grass. Dogs and foxes are different kinds of animals.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "class MarkovChainTextGenerator:\n",
        "    def __init__(self, text, n=2):\n",
        "        \"\"\"\n",
        "        Initialize the Markov Chain text generator.\n",
        "\n",
        "        Parameters:\n",
        "        - text (str): The input text to base the model on.\n",
        "        - n (int): The number of previous words to consider (n-gram model). Default is 2 (bigram model).\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.markov_chain = self.build_markov_chain(text)\n",
        "\n",
        "    def build_markov_chain(self, text):\n",
        "        \"\"\"\n",
        "        Build the Markov chain from the input text.\n",
        "\n",
        "        Parameters:\n",
        "        - text (str): The input text to build the chain from.\n",
        "\n",
        "        Returns:\n",
        "        - markov_chain (dict): A dictionary where keys are tuples of (n-1) words, and values are lists of words\n",
        "          that can follow that tuple, along with their probabilities.\n",
        "        \"\"\"\n",
        "        words = text.split()\n",
        "        markov_chain = {}\n",
        "\n",
        "        # Loop through the words and create n-grams\n",
        "        for i in range(len(words) - self.n + 1):\n",
        "            ngram = tuple(words[i:i + self.n - 1])  # (n-1)-gram (key)\n",
        "            next_word = words[i + self.n - 1]  # The word that follows the n-gram (value)\n",
        "\n",
        "            if ngram not in markov_chain:\n",
        "                markov_chain[ngram] = []\n",
        "            markov_chain[ngram].append(next_word)\n",
        "\n",
        "        return markov_chain\n",
        "\n",
        "    def generate_text(self, length=50, seed=None):\n",
        "        \"\"\"\n",
        "        Generate text using the Markov chain.\n",
        "\n",
        "        Parameters:\n",
        "        - length (int): The number of words to generate.\n",
        "        - seed (str): The starting seed word. If None, the first word from the chain is chosen randomly.\n",
        "\n",
        "        Returns:\n",
        "        - generated_text (str): The generated text.\n",
        "        \"\"\"\n",
        "        if seed is None:\n",
        "            # Pick a random starting point if no seed is given\n",
        "            seed = random.choice(list(self.markov_chain.keys()))[0]\n",
        "\n",
        "        current_tuple = (seed,)\n",
        "        generated_words = [seed]\n",
        "\n",
        "        for _ in range(length - 1):\n",
        "            if current_tuple not in self.markov_chain:\n",
        "                break  # Stop if no possible next word\n",
        "\n",
        "            # Randomly choose the next word based on the transition probabilities\n",
        "            next_word = random.choice(self.markov_chain[current_tuple])\n",
        "            generated_words.append(next_word)\n",
        "\n",
        "            # Update the current tuple to the new n-gram (moving the window)\n",
        "            current_tuple = tuple(generated_words[-(self.n - 1):])\n",
        "\n",
        "        return ' '.join(generated_words)\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample text (can be a larger corpus in real use)\n",
        "    sample_text = \"\"\"\n",
        "    The quick brown fox jumps over the lazy dog. The quick brown fox is very fast.\n",
        "    The lazy dog is sleeping on the grass. Dogs and foxes are different kinds of animals.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the Markov Chain Text Generator with the sample text and 2-gram model (bigram)\n",
        "    generator = MarkovChainTextGenerator(sample_text, n=2)\n",
        "\n",
        "    # Generate text\n",
        "    generated_text = generator.generate_text(length=20, seed=\"The\")\n",
        "    print(generated_text)\n"
      ]
    }
  ]
}