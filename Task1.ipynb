{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjani4jnn/PRODIGY_GA_TASKS/blob/main/Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdEc38safsD1",
        "outputId": "dd0d0ca3-2250-4a57-ea7f-44240ea654e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "viO1xAm1fwX2"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF8ECciSf8LJ",
        "outputId": "692c82b4-4a44-4338-e200-c08d7646a423"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"  # You can also try \"gpt2-medium\", \"gpt2-large\", or \"gpt2-xl\" for larger models\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "owznlj8agHKn"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt, max_length=100, num_return_sequences=1):\n",
        "    # Encode the prompt to tensor tokens\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate text using the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,        # Max number of tokens to generate\n",
        "            num_return_sequences=num_return_sequences,  # Number of sequences to generate\n",
        "            no_repeat_ngram_size=2,       # Avoid repeating n-grams (helpful for diversity)\n",
        "            top_k=50,                     # Sampling strategy: consider the top 50 logits\n",
        "            top_p=0.95,                   # Nucleus sampling (probability mass)\n",
        "            temperature=0.7,              # Control randomness in generation\n",
        "            do_sample=True,               # Sampling instead of greedy search\n",
        "            pad_token_id=tokenizer.eos_token_id  # Padding token (usually eos token)\n",
        "        )\n",
        "\n",
        "    # Decode the generated tokens into text\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "yDiVaOcPgM7x"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt, max_length=100, num_return_sequences=1):\n",
        "    # Encode the prompt to tensor tokens\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate text using the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,        # Max number of tokens to generate\n",
        "            num_return_sequences=num_return_sequences,  # Number of sequences to generate\n",
        "            no_repeat_ngram_size=2,       # Avoid repeating n-grams (helpful for diversity)\n",
        "            top_k=50,                     # Sampling strategy: consider the top 50 logits\n",
        "            top_p=0.95,                   # Nucleus sampling (probability mass)\n",
        "            temperature=0.7,              # Control randomness in generation\n",
        "            do_sample=True,               # Sampling instead of greedy search\n",
        "            pad_token_id=tokenizer.eos_token_id  # Padding token (usually eos token)\n",
        "        )\n",
        "\n",
        "    # Decode the generated tokens into text\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05Deh5FxgQtB",
        "outputId": "640cbfa9-43a8-4d6c-f492-84b447203d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, in a land far, far away, the world was in the midst of a great war, and the enemy had won. The Emperor was now the Emperor of the people, with the greatest power in his hand. His strength was greater than that of any other man, for the victory would never be won by any man who was not his brother, his father, or his mother.\n",
            "\n",
            "The Emperor's strength and wisdom were more than sufficient to defeat the enemies of this world and to make it victorious over all the other worlds. He would bring about the end of all war and peace and of peace between the two peoples of Cyrodiil. This was the time of great danger for humanity and for all of mankind. One day the imperial family of Cyprus was at the head of an expedition, which was sent to the shores of Lake Cyrenia, where the great Cyrene Sea was, under the direction of Prince Phaedra. After the arrival of Ph\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Once upon a time, in a land far, far away,\"\n",
        "\n",
        "generated_text = generate_text(prompt, max_length=200)\n",
        "print(generated_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4wJcpWAIDmRPdZMQFlZ2a",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}